# AI Road Trip Storyteller - Performance Testing Suite

## Overview

This comprehensive performance testing suite validates the AI Road Trip Storyteller application's performance, scalability, and reliability. It includes load testing, stress testing, benchmarking, and real-time monitoring capabilities.

## Test Coverage

### ðŸŽ¯ Load Testing
- **User Behavior Simulation**: 5 distinct user types with realistic interaction patterns
- **Concurrent Users**: Tests from 100 to 2000+ concurrent users
- **Geographic Distribution**: Simulates users from different regions
- **Scenario Coverage**: Voice interactions, AI story generation, booking flows, navigation

### ðŸ”¥ Stress Testing
- **Breaking Point Detection**: Identifies system limits and failure points
- **Spike Testing**: Validates response to sudden traffic surges
- **Cascade Failure Testing**: Tests system resilience during component failures
- **Sustained Load Testing**: Long-duration tests for memory leaks and degradation

### âš¡ Performance Benchmarking
- **API Endpoint Benchmarks**: Response time baselines for all major endpoints
- **Database Query Performance**: Critical query optimization validation
- **Cache Performance**: Redis operation benchmarks and hit rate analysis
- **AI Service Integration**: Voice synthesis and story generation performance

### ðŸ“Š Real-time Monitoring
- **Continuous Metrics Collection**: Response times, error rates, system resources
- **Automated Alerting**: Configurable thresholds with notification system
- **Performance Regression Detection**: Automatic comparison with baselines
- **Prometheus Integration**: Standard metrics format for monitoring tools

## Quick Start

### Prerequisites
```bash
# Install dependencies
pip install -r requirements.txt
pip install locust aiohttp redis psutil matplotlib pandas prometheus_client

# Ensure services are running
docker-compose up -d  # Start PostgreSQL, Redis
uvicorn backend.app.main:app --host 0.0.0.0 --port 8000  # Start API
```

### Run All Performance Tests
```bash
# Comprehensive testing (production validation)
python tests/performance/run_performance_tests.py

# CI-friendly testing (shorter duration)
python tests/performance/run_performance_tests.py --ci-mode

# Custom target URL
python tests/performance/run_performance_tests.py --url https://your-api.com
```

### Run Individual Test Suites

#### Load Testing Only
```bash
python tests/performance/run_performance_tests.py --skip-stress --skip-benchmarks
```

#### Stress Testing Only
```bash
python tests/performance/run_performance_tests.py --skip-load --skip-benchmarks
```

#### Benchmarks Only
```bash
python tests/performance/run_performance_tests.py --skip-load --skip-stress
```

#### Performance Monitoring
```bash
python tests/performance/run_performance_tests.py --enable-monitoring --monitoring-duration 600
```

## Test Scenarios

### Load Test Scenarios

| Scenario | Users | Duration | Purpose |
|----------|-------|----------|---------|
| Baseline Load | 100 | 5 min | Normal expected traffic |
| Peak Hours | 500 | 10 min | Busy period simulation |
| Stress Test | 1000 | 15 min | Beyond normal capacity |
| Spike Test | 2000 | 2 min | Sudden traffic surge |
| Soak Test | 200 | 1 hour | Extended duration |
| Breaking Point | 5000 | 10 min | Find system limits |

### User Behavior Types

1. **Standard Traveler (60%)**: Voice commands, story requests, POI searches
2. **Power Traveler (20%)**: Complex journeys, event planning, AR features
3. **Mobile User (15%)**: Real-time navigation, offline sync, camera features  
4. **Rideshare Driver (5%)**: Driver-specific features, passenger content
5. **Stress Test User**: Aggressive patterns for breaking point detection

### Performance Baselines

| Operation | Target (ms) | Acceptable (ms) | Category |
|-----------|-------------|-----------------|----------|
| Voice Assistant Query | <200 | <500 | Critical |
| AI Story Generation | <2000 | <5000 | Important |
| Get Directions | <150 | <300 | Critical |
| POI Search | <100 | <200 | Critical |
| Hotel Search | <300 | <600 | Important |
| User Lookup | <5 | <10 | Critical |
| Cache Operations | <2 | <5 | Critical |

## Architecture & Components

### Test Framework Components

```
tests/performance/
â”œâ”€â”€ performance_test_framework.py    # Main orchestration framework
â”œâ”€â”€ load_tests/
â”‚   â””â”€â”€ user_behaviors.py           # Realistic user behavior patterns
â”œâ”€â”€ stress_tests/
â”‚   â””â”€â”€ stress_test_scenarios.py    # Breaking point & failure testing
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ performance_benchmarks.py   # API & system benchmarking
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ performance_monitor.py      # Real-time monitoring system
â””â”€â”€ reports/                        # Generated test reports
```

### User Behavior Simulation

Our load testing uses sophisticated user behavior models:

- **Realistic Timing**: Variable wait times between actions
- **Context Awareness**: Location-based interactions
- **Session Continuity**: Persistent user sessions with authentication
- **Geographic Distribution**: Simulated latency by region
- **Mobile vs Desktop**: Different interaction patterns

### Monitoring & Alerting

Real-time performance monitoring includes:

- **Response Time Tracking**: P50, P95, P99 percentiles
- **Error Rate Monitoring**: By endpoint and status code
- **System Resource Tracking**: CPU, memory, disk, network
- **Database Performance**: Connection pool usage, query times
- **Cache Metrics**: Hit rates, memory usage, operations/second

## Results & Reporting

### Automated Reports

All tests generate comprehensive reports including:

1. **Executive Summary**: Pass/fail status, key metrics
2. **Performance Charts**: Response times, throughput, error rates
3. **Baseline Comparisons**: Performance vs. target thresholds
4. **Resource Usage**: CPU, memory, database connections
5. **Recommendations**: Actionable optimization suggestions

### Report Locations

```
tests/performance/reports/
â”œâ”€â”€ benchmark_YYYYMMDD_HHMMSS/      # Benchmark results
â”‚   â”œâ”€â”€ benchmark_results.json
â”‚   â”œâ”€â”€ baseline_comparison.png
â”‚   â””â”€â”€ benchmark_report.html
â”œâ”€â”€ load_test_results.json          # Load testing summary
â”œâ”€â”€ stress_test_report.json         # Stress testing results
â””â”€â”€ performance_test_report_YYYYMMDD_HHMMSS.json  # Final report
```

## Performance Targets

### Production Readiness Criteria

- âœ… **99.9% Uptime**: Error rate <0.1% under normal load
- âœ… **Response Time**: P95 <500ms for critical endpoints
- âœ… **Throughput**: Support 1000+ concurrent users
- âœ… **Scalability**: Handle 10x traffic spikes gracefully
- âœ… **Recovery**: <30s recovery from failures

### Breaking Point Targets

- **Minimum Capacity**: 500 concurrent users before degradation
- **Maximum Tested**: Successfully tested up to 5000 users
- **Graceful Degradation**: No cascade failures during overload
- **Memory Stability**: No memory leaks during 1-hour soak tests

## CI/CD Integration

### GitHub Actions

Add to your workflow:

```yaml
- name: Performance Tests
  run: |
    python tests/performance/run_performance_tests.py --ci-mode
  env:
    DATABASE_URL: ${{ secrets.DATABASE_URL }}
    REDIS_URL: redis://localhost:6379
```

### Performance Regression Detection

The framework automatically compares results with historical baselines:

- **Threshold Alerts**: Automatic failure if performance degrades >20%
- **Trend Analysis**: Track performance over time
- **Branch Comparisons**: Compare feature branches with main

### Docker Integration

```bash
# Run tests in containerized environment
docker run --network="host" -v $(pwd):/app \
  python:3.9 python /app/tests/performance/run_performance_tests.py --ci-mode
```

## Advanced Configuration

### Custom Load Patterns

Create custom user behavior by extending `BaseRoadTripUser`:

```python
class CustomUser(BaseRoadTripUser):
    wait_time = between(1, 3)
    weight = 30  # 30% of users
    
    @task(50)
    def custom_workflow(self):
        # Your custom test logic
        pass
```

### Alert Configuration

Customize performance alerts:

```python
alerts = [
    PerformanceAlert(
        id="custom_alert",
        metric_name="api_response_time_avg",
        threshold=800,  # 800ms
        comparison="gt",
        severity="warning",
        duration_minutes=3,
        description="Custom response time alert"
    )
]
```

### Database Performance Testing

Add custom database benchmarks:

```python
async def custom_db_benchmark():
    with self.db_engine.connect() as conn:
        result = conn.execute(text("YOUR_QUERY_HERE"))
        return result.fetchall()

# Add to benchmark suite
result = await self.run_benchmark(
    "Custom DB Query",
    "Database", 
    custom_db_benchmark,
    iterations=100
)
```

## Troubleshooting

### Common Issues

**High Error Rates**
- Check API authentication tokens
- Verify database connections
- Review rate limiting settings

**Low Performance**
- Increase system resources (CPU/memory)
- Check for database connection pool exhaustion
- Verify cache hit rates

**Test Failures**
- Ensure all services are running
- Check network connectivity
- Review log files for detailed errors

### Performance Optimization Tips

1. **Database Optimization**
   - Add indexes for frequently queried columns
   - Use connection pooling
   - Implement query result caching

2. **API Optimization**
   - Cache expensive operations (AI responses)
   - Implement response compression
   - Use async/await for I/O operations

3. **System Optimization**
   - Horizontal scaling with load balancers
   - CDN for static content
   - Redis clustering for cache scaling

## Monitoring in Production

### Prometheus Integration

The framework exports metrics compatible with Prometheus:

```bash
# Access metrics endpoint
curl http://localhost:9091/metrics
```

### Grafana Dashboards

Import the provided Grafana dashboard template for visualization:

- Response time percentiles
- Error rate trends  
- System resource usage
- Cache performance metrics

### Alerting Rules

Example Prometheus alerting rules:

```yaml
groups:
- name: roadtrip_performance
  rules:
  - alert: HighResponseTime
    expr: api_response_time_seconds{quantile="0.95"} > 0.5
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: High API response time detected
```

## Contributing

### Adding New Tests

1. **User Behaviors**: Add new user types in `load_tests/user_behaviors.py`
2. **Stress Scenarios**: Extend scenarios in `stress_tests/stress_test_scenarios.py`
3. **Benchmarks**: Add new benchmarks in `benchmarks/performance_benchmarks.py`
4. **Monitoring**: Add new metrics in `tools/performance_monitor.py`

### Performance Test Guidelines

- Always include realistic data in test scenarios
- Use appropriate wait times between actions
- Test both success and failure cases
- Include proper cleanup in test teardown
- Document expected performance baselines

### Code Review Checklist

- [ ] Tests cover new API endpoints
- [ ] Realistic user behavior patterns
- [ ] Appropriate performance thresholds
- [ ] Error handling and cleanup
- [ ] Documentation updates

## References

- [Locust Documentation](https://docs.locust.io/)
- [Prometheus Metrics](https://prometheus.io/docs/concepts/metric_types/)
- [Performance Testing Best Practices](https://martinfowler.com/articles/practical-test-pyramid.html)
- [Load Testing Guidelines](https://docs.microsoft.com/en-us/azure/architecture/patterns/load-leveling)

---

**ðŸ“ˆ Performance testing is critical for production readiness. Run these tests regularly to ensure your application scales with user growth!**