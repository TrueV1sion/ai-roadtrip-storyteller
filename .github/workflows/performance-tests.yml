name: Performance Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'ci-mode'
        type: choice
        options:
        - ci-mode
        - full-suite
        - load-only
        - stress-only
        - benchmarks-only
      target_url:
        description: 'Target API URL (optional)'
        required: false
        default: 'http://localhost:8000'

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'

jobs:
  prepare-environment:
    runs-on: ubuntu-latest
    outputs:
      should-run-performance: ${{ steps.check-changes.outputs.performance-relevant }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Check for performance-relevant changes
        id: check-changes
        run: |
          # Check if changes affect performance-critical components
          CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD)
          
          # Define patterns for performance-relevant files
          PERFORMANCE_PATTERNS=(
            "backend/app/"
            "mobile/src/"
            "tests/performance/"
            "docker-compose"
            "requirements"
            ".github/workflows/performance"
          )
          
          PERFORMANCE_RELEVANT=false
          
          for pattern in "${PERFORMANCE_PATTERNS[@]}"; do
            if echo "$CHANGED_FILES" | grep -q "$pattern"; then
              PERFORMANCE_RELEVANT=true
              break
            fi
          done
          
          # Always run on main branch or manual trigger
          if [[ "${{ github.ref }}" == "refs/heads/main" ]] || [[ "${{ github.event_name }}" == "workflow_dispatch" ]] || [[ "${{ github.event_name }}" == "schedule" ]]; then
            PERFORMANCE_RELEVANT=true
          fi
          
          echo "performance-relevant=$PERFORMANCE_RELEVANT" >> $GITHUB_OUTPUT
          echo "Performance-relevant changes: $PERFORMANCE_RELEVANT"

  performance-tests:
    runs-on: ubuntu-latest
    needs: prepare-environment
    if: needs.prepare-environment.outputs.should-run-performance == 'true'
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_DB: roadtrip_test
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: mobile/package-lock.json

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            postgresql-client \
            redis-tools \
            build-essential \
            libpq-dev

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          
          # Additional performance testing dependencies
          pip install locust aiohttp matplotlib pandas prometheus_client

      - name: Install mobile dependencies
        run: |
          cd mobile
          npm ci

      - name: Set up environment variables
        run: |
          cp .env.example .env
          
          # Override with test-specific values
          cat >> .env << EOF
          DATABASE_URL=postgresql://postgres:postgres@localhost:5432/roadtrip_test
          REDIS_URL=redis://localhost:6379
          ENVIRONMENT=testing
          DEBUG=false
          GOOGLE_AI_PROJECT_ID=${{ secrets.GOOGLE_AI_PROJECT_ID }}
          VERTEX_AI_LOCATION=${{ secrets.VERTEX_AI_LOCATION }}
          JWT_SECRET_KEY=test-secret-key-for-performance-testing
          RATE_LIMIT_ENABLED=false
          EOF

      - name: Run database migrations
        run: |
          export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/roadtrip_test
          alembic upgrade head

      - name: Start application server
        run: |
          # Start the API server in background
          uvicorn backend.app.main:app --host 0.0.0.0 --port 8000 &
          API_PID=$!
          echo "API_PID=$API_PID" >> $GITHUB_ENV
          
          # Wait for server to be ready
          timeout 60 bash -c 'until curl -sf http://localhost:8000/api/health; do sleep 2; done'
          
          echo "API server started successfully"

      - name: Verify service health
        run: |
          # Check API health
          curl -f http://localhost:8000/api/health || exit 1
          
          # Check database connection
          psql postgresql://postgres:postgres@localhost:5432/roadtrip_test -c "SELECT 1;" || exit 1
          
          # Check Redis connection
          redis-cli -p 6379 ping || exit 1

      - name: Determine test configuration
        id: test-config
        run: |
          TEST_TYPE="${{ github.event.inputs.test_type || 'ci-mode' }}"
          TARGET_URL="${{ github.event.inputs.target_url || 'http://localhost:8000' }}"
          
          case "$TEST_TYPE" in
            "ci-mode")
              TEST_ARGS="--ci-mode"
              ;;
            "full-suite")
              TEST_ARGS="--enable-monitoring --monitoring-duration 180"
              ;;
            "load-only")
              TEST_ARGS="--skip-stress --skip-benchmarks --ci-mode"
              ;;
            "stress-only")
              TEST_ARGS="--skip-load --skip-benchmarks --ci-mode"
              ;;
            "benchmarks-only")
              TEST_ARGS="--skip-load --skip-stress"
              ;;
            *)
              TEST_ARGS="--ci-mode"
              ;;
          esac
          
          echo "test-args=$TEST_ARGS" >> $GITHUB_OUTPUT
          echo "target-url=$TARGET_URL" >> $GITHUB_OUTPUT
          echo "Test configuration: $TEST_TYPE with args: $TEST_ARGS"

      - name: Run performance tests
        id: performance-tests
        run: |
          echo "Starting performance tests..."
          
          # Set resource limits for CI environment
          ulimit -n 65536  # Increase file descriptor limit
          
          # Run performance tests
          python tests/performance/run_performance_tests.py \
            --url "${{ steps.test-config.outputs.target-url }}" \
            ${{ steps.test-config.outputs.test-args }} \
            --database-url "postgresql://postgres:postgres@localhost:5432/roadtrip_test" \
            --redis-url "redis://localhost:6379" \
            2>&1 | tee performance_test_output.log

      - name: Parse test results
        id: parse-results
        if: always()
        run: |
          # Extract key metrics from the latest report
          LATEST_REPORT=$(find tests/performance/reports -name "performance_test_report_*.json" -type f -exec ls -t {} \; | head -1)
          
          if [[ -f "$LATEST_REPORT" ]]; then
            echo "Found report: $LATEST_REPORT"
            
            # Extract key metrics using jq
            OVERALL_STATUS=$(jq -r '.overall_status' "$LATEST_REPORT")
            DURATION=$(jq -r '.duration_seconds' "$LATEST_REPORT")
            PHASES_COMPLETED=$(jq -r '.summary.phases_completed' "$LATEST_REPORT")
            TOTAL_PHASES=$(jq -r '.summary.total_phases' "$LATEST_REPORT")
            
            echo "overall-status=$OVERALL_STATUS" >> $GITHUB_OUTPUT
            echo "duration=$DURATION" >> $GITHUB_OUTPUT
            echo "phases-completed=$PHASES_COMPLETED" >> $GITHUB_OUTPUT
            echo "total-phases=$TOTAL_PHASES" >> $GITHUB_OUTPUT
            
            # Extract specific metrics if available
            if jq -e '.detailed_results.benchmarks' "$LATEST_REPORT" >/dev/null; then
              BENCHMARK_SUCCESS=$(jq -r '.detailed_results.benchmarks.avg_success_rate // 0' "$LATEST_REPORT")
              echo "benchmark-success-rate=$BENCHMARK_SUCCESS" >> $GITHUB_OUTPUT
            fi
            
            if jq -e '.detailed_results.load_tests' "$LATEST_REPORT" >/dev/null; then
              LOAD_REQUESTS=$(jq -r '.detailed_results.load_tests.total_requests // 0' "$LATEST_REPORT")
              LOAD_ERRORS=$(jq -r '.detailed_results.load_tests.total_errors // 0' "$LATEST_REPORT")
              LOAD_RESPONSE_TIME=$(jq -r '.detailed_results.load_tests.avg_response_time // 0' "$LATEST_REPORT")
              
              echo "load-total-requests=$LOAD_REQUESTS" >> $GITHUB_OUTPUT
              echo "load-total-errors=$LOAD_ERRORS" >> $GITHUB_OUTPUT
              echo "load-avg-response-time=$LOAD_RESPONSE_TIME" >> $GITHUB_OUTPUT
            fi
            
            # Copy report for artifact upload
            cp "$LATEST_REPORT" performance_test_final_report.json
            
          else
            echo "No performance test report found"
            echo "overall-status=ERROR" >> $GITHUB_OUTPUT
          fi

      - name: Upload performance test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results-${{ github.run_number }}
          path: |
            tests/performance/reports/
            performance_test_output.log
            performance_test_final_report.json
          retention-days: 30

      - name: Upload test logs
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: performance-test-logs-${{ github.run_number }}
          path: |
            performance_tests.log
            *.log
          retention-days: 7

      - name: Generate performance summary
        if: always() && steps.parse-results.outputs.overall-status != 'ERROR'
        run: |
          cat << EOF > performance_summary.md
          ## 📊 Performance Test Results
          
          **Overall Status:** ${{ steps.parse-results.outputs.overall-status == 'PASS' && '✅ PASS' || '❌ FAIL' }}
          
          **Test Configuration:**
          - Test Type: ${{ github.event.inputs.test_type || 'ci-mode' }}
          - Target URL: ${{ steps.test-config.outputs.target-url }}
          - Duration: ${{ steps.parse-results.outputs.duration }} seconds
          - Phases Completed: ${{ steps.parse-results.outputs.phases-completed }}/${{ steps.parse-results.outputs.total-phases }}
          
          **Key Metrics:**
          $(if [[ "${{ steps.parse-results.outputs.benchmark-success-rate }}" != "" ]]; then echo "- Benchmark Success Rate: ${{ steps.parse-results.outputs.benchmark-success-rate }}"; fi)
          $(if [[ "${{ steps.parse-results.outputs.load-total-requests }}" != "" ]]; then echo "- Load Test Requests: ${{ steps.parse-results.outputs.load-total-requests }}"; fi)
          $(if [[ "${{ steps.parse-results.outputs.load-total-errors }}" != "" ]]; then echo "- Load Test Errors: ${{ steps.parse-results.outputs.load-total-errors }}"; fi)
          $(if [[ "${{ steps.parse-results.outputs.load-avg-response-time }}" != "" ]]; then echo "- Average Response Time: ${{ steps.parse-results.outputs.load-avg-response-time }}ms"; fi)
          
          **Artifacts:** Check the Actions tab for detailed reports and logs.
          EOF

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const summary = fs.readFileSync('performance_summary.md', 'utf8');
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary
              });
            } catch (error) {
              console.log('Could not post performance summary:', error);
            }

      - name: Cleanup
        if: always()
        run: |
          # Stop the API server
          if [[ -n "$API_PID" ]]; then
            kill $API_PID || true
          fi
          
          # Clean up any remaining processes
          pkill -f "uvicorn" || true

      - name: Fail job if performance tests failed
        if: steps.parse-results.outputs.overall-status == 'FAIL'
        run: |
          echo "Performance tests failed with status: ${{ steps.parse-results.outputs.overall-status }}"
          exit 1

  performance-regression-check:
    runs-on: ubuntu-latest
    needs: performance-tests
    if: github.event_name == 'pull_request' && needs.performance-tests.result == 'success'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download current performance results
        uses: actions/download-artifact@v4
        with:
          name: performance-test-results-${{ github.run_number }}
          path: current-results/

      - name: Download baseline performance results
        continue-on-error: true
        uses: dawidd6/action-download-artifact@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          workflow: performance-tests.yml
          branch: main
          name_is_regexp: true
          name: performance-test-results-.*
          path: baseline-results/
          if_no_artifact_found: warn

      - name: Compare performance results
        id: compare
        run: |
          CURRENT_REPORT=$(find current-results -name "performance_test_report_*.json" -type f | head -1)
          BASELINE_REPORT=$(find baseline-results -name "performance_test_report_*.json" -type f | head -1)
          
          if [[ -f "$CURRENT_REPORT" && -f "$BASELINE_REPORT" ]]; then
            echo "Comparing performance results..."
            
            # Extract key metrics for comparison
            CURRENT_RESPONSE_TIME=$(jq -r '.detailed_results.load_tests.avg_response_time // 0' "$CURRENT_REPORT")
            BASELINE_RESPONSE_TIME=$(jq -r '.detailed_results.load_tests.avg_response_time // 0' "$BASELINE_REPORT")
            
            CURRENT_ERROR_RATE=$(jq -r '.detailed_results.load_tests.total_errors / .detailed_results.load_tests.total_requests * 100 // 0' "$CURRENT_REPORT")
            BASELINE_ERROR_RATE=$(jq -r '.detailed_results.load_tests.total_errors / .detailed_results.load_tests.total_requests * 100 // 0' "$BASELINE_REPORT")
            
            # Calculate regression thresholds (20% degradation)
            RESPONSE_TIME_THRESHOLD=$(echo "$BASELINE_RESPONSE_TIME * 1.2" | bc -l)
            ERROR_RATE_THRESHOLD=$(echo "$BASELINE_ERROR_RATE + 2" | bc -l)  # +2% error rate
            
            REGRESSION_DETECTED=false
            REGRESSION_DETAILS=""
            
            if (( $(echo "$CURRENT_RESPONSE_TIME > $RESPONSE_TIME_THRESHOLD" | bc -l) )); then
              REGRESSION_DETECTED=true
              REGRESSION_DETAILS="Response time regression: ${CURRENT_RESPONSE_TIME}ms vs ${BASELINE_RESPONSE_TIME}ms baseline (+$(echo "scale=1; ($CURRENT_RESPONSE_TIME - $BASELINE_RESPONSE_TIME) / $BASELINE_RESPONSE_TIME * 100" | bc -l)%)"
            fi
            
            if (( $(echo "$CURRENT_ERROR_RATE > $ERROR_RATE_THRESHOLD" | bc -l) )); then
              REGRESSION_DETECTED=true
              REGRESSION_DETAILS="${REGRESSION_DETAILS}\nError rate regression: ${CURRENT_ERROR_RATE}% vs ${BASELINE_ERROR_RATE}% baseline"
            fi
            
            echo "regression-detected=$REGRESSION_DETECTED" >> $GITHUB_OUTPUT
            echo "regression-details=$REGRESSION_DETAILS" >> $GITHUB_OUTPUT
            
          else
            echo "Could not find reports for comparison"
            echo "regression-detected=false" >> $GITHUB_OUTPUT
          fi

      - name: Comment on PR if regression detected
        if: steps.compare.outputs.regression-detected == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const regressionDetails = `${{ steps.compare.outputs.regression-details }}`;
            
            const comment = `
            ## ⚠️ Performance Regression Detected
            
            This PR introduces performance regressions compared to the main branch:
            
            ${regressionDetails}
            
            Please review the performance impact and consider optimization before merging.
            `;
            
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Fail if significant regression
        if: steps.compare.outputs.regression-detected == 'true'
        run: |
          echo "Significant performance regression detected!"
          echo "${{ steps.compare.outputs.regression-details }}"
          exit 1